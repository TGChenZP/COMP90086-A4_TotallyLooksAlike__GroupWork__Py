{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import sys\n",
    "# import os\n",
    "# py_file_location = \"./drive/My Drive/LAB/COMP90086\"\n",
    "# sys.path.append(os.path.abspath(py_file_location))\n",
    "\n",
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from environment import *\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT = './drive/My Drive/LAB/COMP90086/COMP90086_2023_TLLdataset 2'\n",
    "ROOT = './COMP90086_2023_TLLdataset 2'\n",
    "# ROOT = '../autodl-tmp/COMP90086_2023_TLLdataset 2'\n",
    "seed = 19260817\n",
    "IMAGE_SIZE = 224\n",
    "data_list = pd.read_csv(ROOT + '/train.csv')\n",
    "future_list = pd.read_csv(ROOT + '/test_candidates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 744.51it/s]\n",
      "100%|██████████| 2000/2000 [00:02<00:00, 746.68it/s]\n",
      "100%|██████████| 2000/2000 [00:02<00:00, 719.60it/s]\n",
      "100%|██████████| 2000/2000 [00:03<00:00, 606.17it/s]\n"
     ]
    }
   ],
   "source": [
    "imgs = dict()\n",
    "for img in tqdm(os.listdir(ROOT+'/train/left')):\n",
    "  imgs[img] = prepare_image(ROOT+'/train/left/'+img, resize_shape=IMAGE_SIZE)\n",
    "\n",
    "for img in tqdm(os.listdir(ROOT+'/train/right')):\n",
    "  imgs[img] = prepare_image(ROOT+'/train/right/'+img, resize_shape=IMAGE_SIZE)\n",
    "\n",
    "for img in tqdm(os.listdir(ROOT+'/test/left')):\n",
    "  imgs[img] = prepare_image(ROOT+'/test/left/'+img, resize_shape=IMAGE_SIZE)\n",
    "\n",
    "for img in tqdm(os.listdir(ROOT+'/test/right')):\n",
    "  imgs[img] = prepare_image(ROOT+'/test/right/'+img, resize_shape=IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, valtest_list = train_test_split(data_list, test_size=0.3, random_state=seed) # train is 70%\n",
    "val_list, test_list = train_test_split(valtest_list, test_size=0.5, random_state=seed) # val is 15%, test is 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'vit_b_16'\n",
    "\n",
    "for _ in range(2):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "class CNN_Siamise_Triplet_CFG:\n",
    "    num_false               = 19 # how many hard negative to try for each anchor\n",
    "    encoder                 = torchvision.models.vit_b_16(pretrained=True)\n",
    "    freeze_encoder          = True\n",
    "    input_shape             = (1, 3, IMAGE_SIZE, IMAGE_SIZE) if IMAGE_SIZE else (1, 3, 245, 200)\n",
    "    num_mlp_layers          = 2\n",
    "    hidden_dim              = 512\n",
    "    dropout                 = 0.1\n",
    "    res_learning            = False\n",
    "    embed_dim               = 128 # size of the actual embedding for each image\n",
    "    pretrained              = True      \n",
    "    crop_pretrained_linear  = False\n",
    "    final_relu              = False\n",
    "    # ------------------------ #\n",
    "    random_state            = seed\n",
    "    lr                      = 1e-5\n",
    "    loss                    = nn.TripletMarginLoss() \n",
    "    ROOT                    = ROOT # rootpath for loading images\n",
    "    rootpath                = './' # rootpath for saving\n",
    "    # rootpath                = py_file_location\n",
    "    DataFactory_Triplet     = DataFactory_Triplet\n",
    "    DataLoader_Triplet      = DataLoader_Triplet\n",
    "    images                  = imgs\n",
    "    name                    = NAME + '_siamese_triplet'\n",
    "    real_eval_batch_size    = 10\n",
    "    resample                = False\n",
    "    num_random_sample_false = 9\n",
    "\n",
    "cnn_siamise_triplet = CNN_Siamise_Triplet(CNN_Siamise_Triplet_CFG)\n",
    "cnn_siamise_triplet.fit(train_list, val_list, batch_size = 32, epochs=100, patience=8, scheduler=True, grad_clip=False, mark='')\n",
    "\n",
    "cnn_siamise_triplet.load()\n",
    "print('Best Validation:')\n",
    "cnn_siamise_triplet.eval(val_list, batch_size=32, return_loss=False)\n",
    "\n",
    "\n",
    "print('Best Test:')\n",
    "future_test_list = turn_val_into_future(test_list, seed)\n",
    "cnn_siamise_triplet.real_eval(future_test_list, 10)\n",
    "\n",
    "final_out1 = cnn_siamise_triplet.real_eval(future_list, 10)\n",
    "final_out1.to_csv(f'./{NAME}_siamese_triplet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'densenet121'\n",
    "\n",
    "for _ in range(2):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "class CNN_Siamise_Categorisation_CFG:\n",
    "    num_false               = 19 # how many hard negative to try for each anchor\n",
    "    encoder                 = NAME\n",
    "    freeze_encoder          = True\n",
    "    input_shape             = (1, 3, IMAGE_SIZE, IMAGE_SIZE) if IMAGE_SIZE else (1, 3, 245, 200)\n",
    "    num_mlp_layers          = 2\n",
    "    hidden_dim              = 512\n",
    "    dropout                 = 0.1\n",
    "    res_learning            = False\n",
    "    embed_dim               = 128 # size of the actual embedding for each image\n",
    "    pretrained              = True      \n",
    "    crop_pretrained_linear  = True\n",
    "    # ------------------------ #\n",
    "    random_state            = seed\n",
    "    lr                      = 1e-5\n",
    "    loss                    = nn.TripletMarginLoss() \n",
    "    ROOT                    = ROOT # rootpath for loading images\n",
    "    rootpath                = './' # rootpath for saving\n",
    "    # rootpath                = py_file_location\n",
    "    DataFactory_Triplet     = DataFactory_Triplet\n",
    "    DataLoader_Triplet      = DataLoader_Triplet\n",
    "    images                  = imgs\n",
    "    name                    = NAME +'siamese_categorisation'\n",
    "    real_eval_batch_size    = 10\n",
    "    resample                = False\n",
    "    num_random_sample_false = 9\n",
    "\n",
    "cnn_siamise_categorisation = CNN_Siamise_Categorisation(CNN_Siamise_Categorisation_CFG)\n",
    "cnn_siamise_categorisation.fit(train_list, val_list, batch_size = 32, epochs=100, patience=8, scheduler=True, grad_clip=False, mark='')\n",
    "\n",
    "cnn_siamise_categorisation.load()\n",
    "print('Best Validation:')\n",
    "cnn_siamise_categorisation.eval(val_list, batch_size=32, return_loss=False)\n",
    "\n",
    "\n",
    "print('Best Test:')\n",
    "future_test_list = turn_val_into_future(test_list, seed)\n",
    "cnn_siamise_categorisation.real_eval(future_test_list, 10)\n",
    "\n",
    "final_out2 = cnn_siamise_categorisation.real_eval(future_list, 10)\n",
    "final_out2.to_csv(f'./{NAME}_siamese_categorisation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'inception_v3'\n",
    "\n",
    "for _ in range(2):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "class CNN_Key_Query_Triplet_CFG:\n",
    "    num_false               = 19 # how many hard negative to try for each anchor\n",
    "    encoder                 = NAME\n",
    "    freeze_encoder          = True\n",
    "    input_shape             = (64, 3, IMAGE_SIZE, IMAGE_SIZE) if IMAGE_SIZE else (64, 3, 245, 200)\n",
    "    num_mlp_layers          = 2\n",
    "    hidden_dim              = 512\n",
    "    dropout                 = 0.1\n",
    "    res_learning            = False\n",
    "    embed_dim               = 128 # size of the actual embedding for each image\n",
    "    pretrained              = True      \n",
    "    crop_pretrained_linear  = True\n",
    "    final_relu              = False\n",
    "    # ------------------------ #\n",
    "    random_state            = seed\n",
    "    lr                      = 1e-5\n",
    "    loss                    = nn.TripletMarginLoss() \n",
    "    ROOT                    = ROOT # rootpath for loading images\n",
    "    rootpath                = './' # rootpath for saving\n",
    "    # rootpath                = py_file_location\n",
    "    DataFactory_Triplet     = DataFactory_Triplet\n",
    "    DataLoader_Triplet      = DataLoader_Triplet\n",
    "    images                  = imgs\n",
    "    name                    = NAME + '_v3_triplet'\n",
    "    real_eval_batch_size    = 64\n",
    "    resample                = True\n",
    "    num_random_sample_false = 9\n",
    "\n",
    "cnn_key_query_triplet = CNN_Key_Query_Triplet(CNN_Key_Query_Triplet_CFG)\n",
    "cnn_key_query_triplet.fit(train_list, val_list, batch_size = 64, epochs=100, patience=8, scheduler=True, grad_clip=False, mark='')\n",
    "\n",
    "cnn_key_query_triplet.load()\n",
    "print('Best Validation:')\n",
    "cnn_key_query_triplet.eval(val_list, batch_size=128, return_loss=False)\n",
    "\n",
    "\n",
    "print('Best Test:')\n",
    "future_test_list = turn_val_into_future(test_list, seed)\n",
    "cnn_key_query_triplet.real_eval(future_test_list, 10)\n",
    "\n",
    "final_out1 = cnn_key_query_triplet.real_eval(future_list, 10)\n",
    "final_out1.to_csv(f'./{NAME}_key_query_triplet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'resnet'\n",
    "\n",
    "for _ in range(2):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "class CNN_Key_Query_Categorisation_CFG:\n",
    "    num_false               = 1 # how many false samples for each true sample\n",
    "    target                  = 1 # target loss for correct. false will be 1-target\n",
    "    encoder                 = NAME # the name of the model we want to load from torchhub\n",
    "    freeze_encoder          = True\n",
    "    input_shape             = (64, 3, IMAGE_SIZE, IMAGE_SIZE) if IMAGE_SIZE else (64, 3, 245, 200)  # (batch size, 3, [what our loaded in images look like])\n",
    "    num_mlp_layers          = 2 # how many mlp layers in the end. 0 means no mlp and just 1 out layer; n means n mlp + outlayer\n",
    "    hidden_dim              = 512 # hidden dim of mlp\n",
    "    dropout                 = 0.1 # dropout rate of mlp\n",
    "    res_learning            = False # whether to use residual layer in mlp\n",
    "    pretrained              = True # whether to use pretrained encoder\n",
    "    crop_pretrained_linear  = True\n",
    "    # ------------------------ #\n",
    "    random_state            = seed\n",
    "    lr                      = 1e-5\n",
    "    loss                    = nn.BCELoss()   \n",
    "    ROOT                    = ROOT # rootpath for loading images\n",
    "    rootpath                = './' # rootpatbbh for saving models\n",
    "    # rootpath                = py_file_location\n",
    "    DataFactory_Categorisation = DataFactory_Categorisation\n",
    "    DataLoader_Categorisation = DataLoader_Categorisation\n",
    "    images                  = imgs\n",
    "    name                    = NAME + '_categorisation'\n",
    "    real_eval_batch_size    = 64\n",
    "    resample                = True\n",
    "    num_random_sample       = 9\n",
    "\n",
    "\n",
    "cnn_key_query_categorisation = CNN_Key_Query_Categorisation(CNN_Key_Query_Categorisation_CFG)\n",
    "future_test_list = turn_val_into_future(test_list, seed)\n",
    "cnn_key_query_categorisation.real_eval(future_test_list, 21)\n",
    "cnn_key_query_categorisation.fit(train_list, val_list, batch_size = 64, epochs=100, patience=8, scheduler=True, grad_clip=False, mark='')\n",
    "\n",
    "cnn_key_query_categorisation.load()\n",
    "print('Best Validation:')\n",
    "cnn_key_query_categorisation.eval(val_list, batch_size=128, return_loss=False)\n",
    "\n",
    "print('Best Test:')\n",
    "future_test_list = turn_val_into_future(test_list, seed)\n",
    "cnn_key_query_categorisation.real_eval(future_test_list, 100)\n",
    "\n",
    "final_out2 = cnn_key_query_categorisation.real_eval(future_list, 100)\n",
    "final_out2.to_csv(f'./{NAME}_key_query_categorisation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
